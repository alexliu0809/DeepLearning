{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import edf\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "train_data, trcnt = utils.load_data_onechar('data/ptb.train.txt')\n",
    "valid_data, vacnt = utils.load_data_onechar('data/ptb.valid.txt')\n",
    "test_data, tecnt = utils.load_data_onechar('data/ptb.test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HINT\n",
    "Let's see inside of the data. train_data is list of char index.\n",
    "$$\n",
    "    \\forall i \\ \\ \\texttt{train_data[i]} \\in Z^{|s|}\n",
    "$$\n",
    "where $s$ is a sequence of charactors. And\n",
    "$$\n",
    "    s \\in \\{c \\in C\\}\n",
    "$$\n",
    "$$\n",
    "    C \\in \\{a,b,c,...,` `,`\\{`, `\\}`\\}\n",
    "$$\n",
    "where \"{\" and \"}\" are special tags for head and end of sentences.\n",
    "\n",
    "( NOTE ) we do NOT use pos-tag or any language information on PTB except sequence of charactors at all! \n",
    "and <font color=\"red\"> you might realize that the word $\"<UNK>\"$ makes strange charactor sequecne $\"<,U,N,K,>\"$. Let's think there is a word with $\"<UNK>\"$ in this world!  lol\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charactor mapping : \n",
      "['@', u'{', u'a', u'e', u'r', u' ', u'b', u'n', u'k', u'o', u't', u'l', u'i', u'z', u'c', u'w', u'y', u'u', u's', u'f', u'm', u'g', u'h', u'd', u'-', u'q', u'p', u'x', u'}', u'<', u'>', u'j', u'v', u'.', u\"'\", u'1', u'9', u'5', u'0', u'&', u'$', u'3', u'2', u'4', u'8', u'6', u'7', u'#', u'\\\\', u'/', u'*']\n",
      "samples : \n",
      "indeces :  [ 1 21  9 32  3  4  7 20  3  7 10  5 14  9  7 10  4  2 14 10  9  4 18 28]\n",
      "real chars :  { g o v e r n m e n t   c o n t r a c t o r s }\n",
      "indeces :  [ 1 10 22  2 10  5 12 18  5 19  9  4  5 10 22  3  5 19 17 10 17  4  3 28]\n",
      "real chars :  { t h a t   i s   f o r   t h e   f u t u r e }\n",
      "indeces :  [ 1 18 10 12 11 11  5 14  2 17 10 12  9  7  5 12 18  5 29 17  7  8 30 28]\n",
      "real chars :  { s t i l l   c a u t i o n   i s   < u n k > }\n",
      "indeces :  [ 1 18 10  9 14  8 18  5 32  9 11 17 20  3  5  7  5 18 22  2  4  3 18 28]\n",
      "real chars :  { s t o c k s   v o l u m e   n   s h a r e s }\n",
      "indeces :  [ 1 22  3  5 15  9 17 11 23  5  7 34 10  5  3 11  2  6  9  4  2 10  3 28]\n",
      "real chars :  { h e   w o u l d   n ' t   e l a b o r a t e }\n"
     ]
    }
   ],
   "source": [
    "#===========================\n",
    "#          HINT\n",
    "#===========================\n",
    "#Charactor mapings\n",
    "chars = [None]*len(utils.vocab.keys())\n",
    "for k in utils.vocab.keys(): chars[utils.vocab[k]]=k\n",
    "print \"Charactor mapping : \"\n",
    "print chars\n",
    "\n",
    "#inside of data\n",
    "samples = train_data[1000:1005]\n",
    "print \"samples : \"\n",
    "for i,s in enumerate(samples):\n",
    "    print \"indeces : \",s\n",
    "    print \"real chars : \",\n",
    "    for c in s:\n",
    "        print chars[c],\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HINT\n",
    "## Probabilities of LSTM model\n",
    "Here let's see what we want to code. We need LSTM which can predict next charactor from the previous sequece, i.e.,\n",
    "$$\n",
    "    {\\rm LSTM} : C^{t-1} \\rightarrow C\n",
    "$$\n",
    "thus LSTM gives probability distribution over $C$,\n",
    "$$\n",
    "    P_{{\\rm LSTM}}( s_t | s_{<t} ) = softmax(V_{[s_t]}h_t)   \n",
    "$$\n",
    "where $V_{[s_t]}$ is another parameter of vector for word $s_t$, $h_t$ is $t$-th output from the LSTM, $s_i$ is $i$-th charactor in the sentence $s$ and $s_{<t}$ is charactors before $t$.\n",
    "\n",
    "Then log likelihood of a sentence is \n",
    "$$\n",
    "    \\log \\mathcal{L} = \\sum_{0<=t<T} \\log P_{{\\rm LSTM}}(s_t| s_{<t})\n",
    "$$\n",
    "<font color=\"red\"> NOTE) we also want to predict the first charactor of a sentence which is always \"{\" and the last one which is \"}\".</font>\n",
    "\n",
    "\n",
    "## Implementation tricks\n",
    "We train the LSTM with dynamically constructing RNN sequence for each batch (we do not stick to a statistic computational graph anymore), i.e.,\n",
    "\n",
    "FOR each batch, $b$ :  \n",
    "| ${\\tt loss, score = buildModel()}$  \n",
    "| ${\\tt edf.Forward()}$  \n",
    "| ${\\tt edf.Backward(loss)}$  \n",
    "| ${\\tt edf.GradClip(10)}$  \n",
    "| ${\\tt edf.SGD(eta)}$  \n",
    "END\n",
    "\n",
    "Now ${\\tt buildModel()}$ is\n",
    "\n",
    "${\\tt buildModel()}$  \n",
    "| Global_var : ${\\tt inp}$, ${\\tt C2V}$, ${\\tt hidden\\_dim}$, $LSTM\\_PARAS$   \n",
    "| ${\\tt loss\\_list = []}$  #List of logL at each time $t$   \n",
    "| ${\\tt score= []}$       #List of probabilities at each time $t$   \n",
    "| $\\tt FOR\\ t<T$ :  \n",
    "| | chain LSTM_CELL  \n",
    "| | compute logLoss,$\\tt l$, and prob,$\\tt p$, at time $\\tt t$  \n",
    "| | ${\\tt loss\\_list += [l]}$  \n",
    "| | ${\\tt score += [p]}$   \n",
    "| $\\tt END$   \n",
    "| ${\\tt loss = average(loss\\_list)}$   \n",
    "| ${\\tt return\\ loss, score}$   \n",
    "$\\tt END$  \n",
    "\n",
    "Again $\\tt score$ is a list of probabilities on each time step $t$ and $\\tt loss$ is a log likelihood of the batch.\n",
    "\n",
    "\n",
    "## Other things you might concern \n",
    "### computational graph\n",
    "Now computational graph is not static, we need to initialize everytime.\n",
    "\n",
    "### initial state of LSTM cell \n",
    "We can think several reasonable implementation but we employed just zero vector.\n",
    "\n",
    "### masks and paddings\n",
    "We train model with mini-batch but LSTM chain has a fix size for all sentences in a batch. We need to remove output from padded part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 49.37356 Avg loss = 3.93269\n",
      "Initial generated sentence \n",
      "the agreements bringun1fffff{myeiieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieieie<zn#lt<<.2#ll66{6y{iffff{myeieieiei\n",
      "0 -th epoch\n",
      "[ 3.90744042  3.98190022  3.95454621  3.89998221]\n",
      "[[ 3.90744042]\n",
      " [ 3.98190022]\n",
      " [ 3.95454621]\n",
      " [ 3.89998221]]\n",
      "[ 3.90480028  3.93716897  3.91379222  3.91539979]\n",
      "[[ 3.90480028]\n",
      " [ 3.93716897]\n",
      " [ 3.91379222]\n",
      " [ 3.91539979]]\n",
      "[ 3.94551179  3.95401026  3.9995839   3.99814916]\n",
      "[[ 3.94551179]\n",
      " [ 3.95401026]\n",
      " [ 3.9995839 ]\n",
      " [ 3.99814916]]\n",
      "[ 3.9420608   3.94391443  3.94255445  3.92786874]\n",
      "[[ 3.9420608 ]\n",
      " [ 3.94391443]\n",
      " [ 3.94255445]\n",
      " [ 3.92786874]]\n",
      "[ 3.92362923  3.92762314  3.93848426  3.99557313]\n",
      "[[ 3.92362923]\n",
      " [ 3.92762314]\n",
      " [ 3.93848426]\n",
      " [ 3.99557313]]\n",
      "[ 4.02056157  3.93862357  3.95104326  3.95169526]\n",
      "[[ 4.02056157]\n",
      " [ 3.93862357]\n",
      " [ 3.95104326]\n",
      " [ 3.95169526]]\n",
      "[ 3.97989117  4.03581478  3.98737721  4.01268376]\n",
      "[[ 3.97989117]\n",
      " [ 4.03581478]\n",
      " [ 3.98737721]\n",
      " [ 4.01268376]]\n",
      "[ 4.06344231  3.99474114  3.96586786  3.97613015]\n",
      "[[ 4.06344231]\n",
      " [ 3.99474114]\n",
      " [ 3.96586786]\n",
      " [ 3.97613015]]\n",
      "[ 4.02648905  3.98612946  4.06048272  3.99676948]\n",
      "[[ 4.02648905]\n",
      " [ 3.98612946]\n",
      " [ 4.06048272]\n",
      " [ 3.99676948]]\n",
      "[ 4.01823931  3.98423075  4.04504757  3.98768724]\n",
      "[[ 4.01823931]\n",
      " [ 3.98423075]\n",
      " [ 4.04504757]\n",
      " [ 3.98768724]]\n",
      "[ 4.0311126   4.0441516   4.09229805  4.08789181]\n",
      "[[ 4.0311126 ]\n",
      " [ 4.0441516 ]\n",
      " [ 4.09229805]\n",
      " [ 4.08789181]]\n",
      "[ 4.05043118  4.03995988  4.03599472  4.01739367]\n",
      "[[ 4.05043118]\n",
      " [ 4.03995988]\n",
      " [ 4.03599472]\n",
      " [ 4.01739367]]\n",
      "[ 3.90935187  3.89970392  3.89913076  4.04645825]\n",
      "[[ 3.90935187]\n",
      " [ 3.89970392]\n",
      " [ 3.89913076]\n",
      " [ 4.04645825]]\n",
      "[ 4.11235907  4.05334032  4.08414732  4.05952041]\n",
      "[[ 4.11235907]\n",
      " [ 4.05334032]\n",
      " [ 4.08414732]\n",
      " [ 4.05952041]]\n",
      "[ 4.08110646  4.05061198  4.05797295  4.05617373]\n",
      "[[ 4.08110646]\n",
      " [ 4.05061198]\n",
      " [ 4.05797295]\n",
      " [ 4.05617373]]\n",
      "[ 4.05790633  4.06762482  4.05966154  4.10469425]\n",
      "[[ 4.05790633]\n",
      " [ 4.06762482]\n",
      " [ 4.05966154]\n",
      " [ 4.10469425]]\n",
      "[ 4.09504272  4.12033688  4.12142511  4.09128689]\n",
      "[[ 4.09504272]\n",
      " [ 4.12033688]\n",
      " [ 4.12142511]\n",
      " [ 4.09128689]]\n",
      "[ 4.10172834  4.1047987   4.17660838  4.15364495]\n",
      "[[ 4.10172834]\n",
      " [ 4.1047987 ]\n",
      " [ 4.17660838]\n",
      " [ 4.15364495]]\n",
      "[ 4.11555805  4.16488705  4.17857027  4.15126866]\n",
      "[[ 4.11555805]\n",
      " [ 4.16488705]\n",
      " [ 4.17857027]\n",
      " [ 4.15126866]]\n",
      "[ 4.17645487  4.16977885  4.14884551  4.13470822]\n",
      "[[ 4.17645487]\n",
      " [ 4.16977885]\n",
      " [ 4.14884551]\n",
      " [ 4.13470822]]\n",
      "[ 4.20894541  4.20567563  4.20316419  4.22692576]\n",
      "[[ 4.20894541]\n",
      " [ 4.20567563]\n",
      " [ 4.20316419]\n",
      " [ 4.22692576]]\n",
      "[ 4.21952431  4.23908819  4.2300779   4.27574506]\n",
      "[[ 4.21952431]\n",
      " [ 4.23908819]\n",
      " [ 4.2300779 ]\n",
      " [ 4.27574506]]\n",
      "[ 4.26722793  4.25752754  4.26368251  4.30658233]\n",
      "[[ 4.26722793]\n",
      " [ 4.25752754]\n",
      " [ 4.26368251]\n",
      " [ 4.30658233]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-b2c47aecc8aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takeshi.onishi/workspace/foundation-of-deep-learning/Version2/ps4/edf.py\u001b[0m in \u001b[0;36mForward\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Global forward/backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takeshi.onishi/workspace/foundation-of-deep-learning/Version2/ps4/edf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "#debug\n",
    "batch=4\n",
    "hidden_dim=10\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "edf.params = []\n",
    "\n",
    "# LSTM parameters\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "    \n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "# for sake of saving\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V])\n",
    "\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "# LSTM cell\n",
    "def LSTMCell(xt, h, c):\n",
    "    \n",
    "    f = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wf), bf))\n",
    "    i = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wi), bi))\n",
    "    o = edf.Sigmoid(edf.Add(edf.VDot(edf.ConCat(xt, h), Wo), bo))\n",
    "    c_hat = edf.Tanh(edf.Add(edf.VDot(edf.ConCat(xt, h), Wc), bc))\n",
    "    c_next = edf.Add(edf.Mul(f, c), edf.Mul(i, c_hat))\n",
    "    h_next = edf.Mul(o, edf.Tanh(c_next))\n",
    "            \n",
    "    return h_next, c_next\n",
    "\n",
    "# build the model given the input inp, it shoudl return loss and prob\n",
    "def BuildModel():\n",
    "    '''\n",
    "    Global vars:\n",
    "        inp : Z^{batch_size x seq_size}\n",
    "    '''\n",
    "    \n",
    "    #initialize CP\n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0] #batch_size\n",
    "    T = inp.value.shape[1] #length of sequence (max len(s))\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) #initial state of the cell\n",
    "    c = edf.Value(np.zeros((B, hidden_dim)))\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) #word embedings on time t R^{batch_size x hidden_dim}\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])         #!!UNNECESSARY RESHAPE!!\n",
    "        h_next, c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        l = edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))) #R^{batch_size}\n",
    "        logloss = edf.Reshape(l, (B, 1))   #R^{batch_size x 1}\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss) #R^{batch_size x T}\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next\n",
    "        c = c_next\n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score,[l, logloss]\n",
    "    \n",
    "    \n",
    "# calculate the perplexity         \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "\n",
    "# predict the sequence\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTMCell(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score,_ = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    print ep,\"-th epoch\"\n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score,debug = BuildModel()\n",
    "        edf.Forward()\n",
    "        for v in debug: print v.value\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-a2e9fbb1b5de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;31m# initial Perplexity and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0mperp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvacnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial: Perplexity: %0.5f Avg loss = %0.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mperp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-a2e9fbb1b5de>\u001b[0m in \u001b[0;36mEval\u001b[0;34m(data, cnt)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mperp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mCalPerp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takeshi.onishi/workspace/foundation-of-deep-learning/Version2/ps4/edf.py\u001b[0m in \u001b[0;36mForward\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Global forward/backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/takeshi.onishi/workspace/foundation-of-deep-learning/Version2/ps4/edf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_dim = 200\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_GRU.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "# GRU parameters\n",
    "edf.params = []\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "Wz = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "Wr = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "W = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "\n",
    "# for sake of saving\n",
    "parameters.extend([C2V, Wz, Wr, W, V])\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "# GRU cell                \n",
    "def GRUCell(xt, h):\n",
    "    \n",
    "    z = edf.Sigmoid(edf.VDot(edf.ConCat(xt, h), Wz))\n",
    "    r = edf.Sigmoid(edf.VDot(edf.ConCat(xt, h), Wr))\n",
    "    h_hat = edf.Tanh(edf.VDot(edf.ConCat(xt, edf.Mul(r, h)), W))\n",
    "    h_next = edf.Add(edf.Mul(z, h_hat), edf.Mul(h, edf.Add(edf.Value(1), edf.Mul(z, edf.Value(-1)))))\n",
    "    \n",
    "    return h_next\n",
    "\n",
    "\n",
    "# build the model given the input inp, it shoudl return loss and prob\n",
    "def BuildModel():\n",
    "    '''\n",
    "    Global vars:\n",
    "        inp : Z^{batch_size x seq_size}\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #initialize CP\n",
    "    edf.components = []\n",
    "\n",
    "    B = inp.value.shape[0] #batch_size\n",
    "    T = inp.value.shape[1] #length of sequence (max len(s))\n",
    "    h = edf.Value(np.zeros((B, hidden_dim))) #initial state of the cell\n",
    "    \n",
    "    score = []\n",
    "    \n",
    "    for t in range(T-1):\n",
    " \n",
    "        wordvec = edf.Embed(edf.Value(inp.value[:,t]), C2V) \n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next = GRUCell(xt, h)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        logloss = edf.Reshape(edf.LogLoss(edf.Aref(p, edf.Value(inp.value[:,t+1]))), (B, 1))\n",
    "        \n",
    "        if t == 0:\n",
    "            loss = logloss\n",
    "        else:\n",
    "            loss = edf.ConCat(loss, logloss)\n",
    "            \n",
    "        score.append(p)    \n",
    "        h = h_next \n",
    "    \n",
    "    masks = np.zeros((B, T-1), dtype = np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    loss = edf.MeanwithMask(loss, edf.Value(masks)) \n",
    "    \n",
    "    return loss, score\n",
    "    \n",
    "\n",
    "# calculate the perplexity     \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "\n",
    "# predict the sequence\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next = GRUCell(xt, h)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "           \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx  \n",
    "\n",
    "    \n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "\n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
