{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import edf\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "train_data, trcnt = utils.load_data_onechar('data/ptb.train.txt')\n",
    "valid_data, vacnt = utils.load_data_onechar('data/ptb.valid.txt')\n",
    "test_data, tecnt = utils.load_data_onechar('data/ptb.test.txt')\n",
    "\n",
    "#train_data, trcnt = utils.load_data_onechar('data/ptb.train.short.txt')\n",
    "#valid_data, vacnt = utils.load_data_onechar('data/ptb.valid.short.txt')\n",
    "#test_data, tecnt = utils.load_data_onechar('data/ptb.test.short.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: Perplexity: 49.16116 Avg loss = 3.88498\n",
      "Initial generated sentence \n",
      "the agreements bringwx*//659e&@$$vl77#*b8bffdx<p3u7dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp37dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp37dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp37dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp37dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp37dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp37dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp37dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp37dm77dmx3h77dmal77#*b/655zssd<>yyy.wxp\n",
      "Epoch 0: Perplexity: 8.42353 Avg loss = 2.15851 [17.282 mins]\n",
      "Epoch 0: generated sentence \n",
      "the agreements bringe to the are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are are\n",
      "Epoch 1: Perplexity: 6.80625 Avg loss = 1.92909 [17.358 mins]\n",
      "Epoch 1: generated sentence \n",
      "the agreements bringer ane onere ane on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on to net on \n",
      "Epoch 2: Perplexity: 5.93165 Avg loss = 1.80426 [17.525 mins]\n",
      "Epoch 2: generated sentence \n",
      "the agreements bringen on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to nee on to n\n",
      "Epoch 3: Perplexity: 5.29035 Avg loss = 1.69236 [18.289 mins]\n",
      "Epoch 3: generated sentence \n",
      "the agreements bringer to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
      "Epoch 4: Perplexity: 4.92081 Avg loss = 1.61965 [18.120 mins]\n",
      "Epoch 4: generated sentence \n",
      "the agreements bringer to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
      "Epoch 5: Perplexity: 4.66637 Avg loss = 1.56870 [17.089 mins]\n",
      "Epoch 5: generated sentence \n",
      "the agreements bringer to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee\n",
      "Epoch 6: Perplexity: 4.49200 Avg loss = 1.53036 [17.081 mins]\n",
      "Epoch 6: generated sentence \n",
      "the agreements bringer of too to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee\n",
      "Epoch 7: Perplexity: 4.30076 Avg loss = 1.48789 [17.039 mins]\n",
      "Epoch 7: generated sentence \n",
      "the agreements bringered to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to \n",
      "Epoch 8: Perplexity: 4.20636 Avg loss = 1.45757 [17.097 mins]\n",
      "Epoch 8: generated sentence \n",
      "the agreements bring to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to t\n",
      "Epoch 9: Perplexity: 4.06869 Avg loss = 1.43384 [18.527 mins]\n",
      "Epoch 9: generated sentence \n",
      "the agreements bring to teesentee interesteees one other to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee otte too to tee to teee\n",
      "Epoch 10: Perplexity: 4.02437 Avg loss = 1.41457 [17.074 mins]\n",
      "Epoch 10: generated sentence \n",
      "the agreements bringer interestion to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to te\n",
      "Epoch 11: Perplexity: 3.91885 Avg loss = 1.39309 [17.246 mins]\n",
      "Epoch 11: generated sentence \n",
      "the agreements bring to teesentee it is too to tee to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to te\n",
      "Epoch 12: Perplexity: 3.86897 Avg loss = 1.38348 [17.100 mins]\n",
      "Epoch 12: generated sentence \n",
      "the agreements bringing to teesentee to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to \n",
      "Epoch 13: Perplexity: 3.82643 Avg loss = 1.36468 [17.006 mins]\n",
      "Epoch 13: generated sentence \n",
      "the agreements bringing to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee to teee \n",
      "Epoch 14: Perplexity: 3.76015 Avg loss = 1.35516 [17.775 mins]\n",
      "Epoch 14: generated sentence \n",
      "the agreements bringer earned to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to\n",
      "Epoch 15: Perplexity: 3.78083 Avg loss = 1.34574 [17.750 mins]\n",
      "Epoch 15: generated sentence \n",
      "the agreements bring to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to \n",
      "Epoch 16: Perplexity: 3.69212 Avg loss = 1.33569 [17.306 mins]\n",
      "Epoch 16: generated sentence \n",
      "the agreements bringing to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet \n",
      "Epoch 17: Perplexity: 3.63581 Avg loss = 1.32589 [17.050 mins]\n",
      "Epoch 17: generated sentence \n",
      "the agreements bringing to tele to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet \n",
      "Epoch 18: Perplexity: 3.61505 Avg loss = 1.31490 [17.088 mins]\n",
      "Epoch 18: generated sentence \n",
      "the agreements bringing othee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oit ottee ottee othee oits tere oi\n",
      "Epoch 19: Perplexity: 3.58312 Avg loss = 1.30864 [17.453 mins]\n",
      "Epoch 19: generated sentence \n",
      "the agreements bringing othee oits tere oits to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to \n",
      "Epoch 20: Perplexity: 3.56877 Avg loss = 1.30202 [17.272 mins]\n",
      "Epoch 20: generated sentence \n",
      "the agreements bringing oits to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to teet to \n",
      "Epoch 21: Perplexity: 3.54229 Avg loss = 1.29324 [17.221 mins]\n",
      "Epoch 21: generated sentence \n",
      "the agreements bringing othee oits tere eet it interesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenesteee it intenest\n",
      "Epoch 22: Perplexity: 3.53403 Avg loss = 1.28060 [17.071 mins]\n",
      "Epoch 22: generated sentence \n",
      "the agreements bringing othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits tere eet ottee othee oits t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b5afc4fff3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m#print(inp.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0medf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Enze/Dropbox/TTIC/PS4/mine/edf.py\u001b[0m in \u001b[0;36mBackward\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Optimization functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Enze/Dropbox/TTIC/PS4/mine/edf.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_dim = 200 #hidden size\n",
    "n_vocab = utils.n_vocab\n",
    "batch = 50\n",
    "parameters = []\n",
    "model = 'model_LSTM.pkl'\n",
    "eta = 0.5\n",
    "decay = 0.9\n",
    "\n",
    "inp = edf.Value()\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "edf.params = []\n",
    "# LSTM parameters\n",
    "# input embedding\n",
    "C2V = edf.Param(edf.xavier((n_vocab, hidden_dim)))\n",
    "# forget gate\n",
    "Wf = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bf = edf.Param(np.zeros((hidden_dim)))\n",
    "# input gate\n",
    "Wi = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bi = edf.Param(np.zeros((hidden_dim)))\n",
    "# carry cell\n",
    "Wc = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bc = edf.Param(np.zeros((hidden_dim)))\n",
    "# output cell\n",
    "Wo = edf.Param(edf.xavier((2*hidden_dim, hidden_dim)))\n",
    "bo = edf.Param(np.zeros((hidden_dim)))\n",
    "# output embedding\n",
    "V = edf.Param(edf.xavier((hidden_dim, n_vocab)))\n",
    "# for sake of saving\n",
    "parameters.extend([C2V, Wf, bf, Wi, bi, Wc, bc, Wo, bo, V])\n",
    "#https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "#Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden. C2V\n",
    "#Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden Wi,Wf,Wc,Wo\n",
    "#Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output  V\n",
    "\n",
    "# load the trained model if exist\n",
    "if os.path.exists(model):\n",
    "    with open(model, 'rb') as f:\n",
    "        p_value = pickle.load(f)\n",
    "        idx = 0\n",
    "        for p in p_value:\n",
    "            parameters[idx].value = p\n",
    "            idx += 1\n",
    "                    \n",
    "\n",
    "# Please finish your LSTM cell in this function. it will build the model given the input inp, it should\n",
    "# return loss and prob score\n",
    "def LSTM(x,cprev, hprev): #previous c and previous h\n",
    "    #reference: CS231 LSTM\n",
    "    #print(x.shape)\n",
    "    #print(hprev.shape)\n",
    "    f = edf.Sigmoid(edf.Add(bf,edf.VDot(edf.ConCat(x,hprev),Wf))) #should this be vdot?\n",
    "    i = edf.Sigmoid(edf.Add(bi,edf.VDot(edf.ConCat(x,hprev),Wi))) #should this be vdot? and the order of multiplication\n",
    "    g = edf.Tanh(edf.Add(bc,edf.VDot(edf.ConCat(x,hprev),Wc))) # g here is carry. should this be sigmoid?\n",
    "    o = edf.Sigmoid(edf.Add(bo,edf.VDot(edf.ConCat(x,hprev),Wo))) #should this be vdot?\n",
    "    #print(i.x.shape,g.x.shape)\n",
    "    c = edf.Add(edf.Mul(f,cprev),edf.Mul(i,g))\n",
    "    h = edf.Mul(o,edf.Tanh(c))\n",
    "    \n",
    "    return c,h\n",
    "\n",
    "def BuildModel():\n",
    " \n",
    "    edf.components = []\n",
    "    score = []\n",
    "    loss = edf.Value(edf.DT(0))\n",
    "    B,T = inp.value.shape #batch size, sentence length\n",
    "    \n",
    "    c0 = edf.Value(np.random.randn(B,hidden_dim))\n",
    "    h0 = edf.Value(np.random.randn(B,hidden_dim))\n",
    "    \n",
    "    #c = []\n",
    "    #h = []\n",
    "    for i in range(T-1):\n",
    "        x = edf.Reshape(edf.Embed(edf.Value(inp.value[:,i]), C2V),[-1, hidden_dim])\n",
    "        c0,h0 = LSTM(x,c0,h0)\n",
    "        #print(xt.value.shape)\n",
    "        #print(c0.value.shape)\n",
    "        #print(h0.value.shape)\n",
    "        \n",
    "        prediction =  edf.SoftMax(edf.VDot(h0, V)) #Dot or Mul\n",
    "        \n",
    "        #loss func reference: mingda chen.\n",
    "        score.append(prediction)\n",
    "        mask = np.zeros(B * n_vocab)\n",
    "        idx = np.int32(inp.value[:,i+1])\n",
    "        mask_mask = [i*n_vocab+j for (i, j) in zip(range(B),idx) if j!=0]\n",
    "        mask[mask_mask] = 1\n",
    "        mask = edf.Value(mask.reshape(B, n_vocab))\n",
    "        loss = edf.Add(edf.MeanwithMask(edf.LogLoss(prediction),mask),loss)\n",
    "        \n",
    "    loss = edf.Mul(loss,edf.Value(1./T))\n",
    "    #wrand = np.random.randn(*score.shape)\n",
    "    #loss = np.sum(score * wrand)\n",
    "        \n",
    "    return loss, score\n",
    "    \n",
    "    \n",
    "# calculate the perplexity         \n",
    "def CalPerp(score):\n",
    "    \n",
    "    prob = [p.value for p in score]\n",
    "    prob = np.transpose(np.stack(prob, axis = 0),(1,0,2))\n",
    "    \n",
    "    B = prob.shape[0]\n",
    "    T = prob.shape[1]\n",
    "    V = prob.shape[2]\n",
    "    \n",
    "    masks = np.zeros((B, T), dtype=np.int32)\n",
    "    masks[inp.value[:,1:] != 0] = 1\n",
    "    \n",
    "    prob = prob.reshape(-1)\n",
    "    idx = np.int32(inp.value[:,1:].reshape(-1))\n",
    "    outer_dim = len(idx)\n",
    "    inner_dim = len(prob)/outer_dim\n",
    "    pick = np.int32(np.array(range(outer_dim))*inner_dim + idx)\n",
    "    prob = prob[pick].reshape(B, T)\n",
    "        \n",
    "    return -np.sum(np.log(prob[np.nonzero(prob*masks)]))\n",
    "\n",
    "\n",
    "# predict the sequence\n",
    "def Predict(max_step, prefix):\n",
    "   \n",
    "    edf.components = []\n",
    "\n",
    "    T = max_step       \n",
    "    h = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    c = edf.Value(np.zeros((1, hidden_dim))) \n",
    "    \n",
    "    prediction = []\n",
    "\n",
    "    for t in range(T):\n",
    "   \n",
    "        if t < len(prefix):\n",
    "            pred = edf.Value(prefix[t])\n",
    "            prediction.append(pred)              \n",
    "        else:\n",
    "            prediction.append(pred)\n",
    "\n",
    "        wordvec = edf.Embed(pred, C2V)\n",
    "        xt = edf.Reshape(wordvec, [-1, hidden_dim])\n",
    "        h_next,c_next = LSTM(xt, h, c)\n",
    "        p = edf.SoftMax(edf.VDot(h_next, V))\n",
    "        pred = edf.ArgMax(p)\n",
    "        h = h_next\n",
    "        c = c_next   \n",
    "            \n",
    "    edf.Forward()\n",
    "    \n",
    "    idx = [pred.value for pred in prediction]\n",
    "    stop_idx = utils.to_index('}')\n",
    "    \n",
    "    if stop_idx in idx:\n",
    "        return idx[0:idx.index(stop_idx)+1]\n",
    "    else:\n",
    "        return idx\n",
    "\n",
    "def Eval(data, cnt):\n",
    "    \n",
    "    perp = 0.\n",
    "    avg_loss = 0.\n",
    "    test_batches = range(0, len(data), batch)\n",
    "    test_minbatches = [data[idx:idx+batch] for idx in test_batches]\n",
    "    \n",
    "    for minbatch in test_minbatches:\n",
    "        \n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel()\n",
    "        edf.Forward()\n",
    "        avg_loss += loss.value\n",
    "        perp += CalPerp(score)\n",
    "           \n",
    "    perp = np.exp(perp/cnt)\n",
    "    avg_loss /= len(test_batches)\n",
    "    return perp, avg_loss\n",
    "\n",
    "\n",
    "############################################### training loop #####################################################\n",
    "\n",
    "batches = range(0, len(train_data), batch)\n",
    "minbatches = [train_data[idx:idx+batch] for idx in batches]\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "# initial Perplexity and loss\n",
    "perp, loss = Eval(valid_data, vacnt)\n",
    "print(\"Initial: Perplexity: %0.5f Avg loss = %0.5f\" % (perp, loss))    \n",
    "best_loss = loss\n",
    "prefix = 'the agreements bring'  \n",
    "generation = Predict(400, utils.to_idxs(prefix))\n",
    "print(\"Initial generated sentence \")\n",
    "print (utils.to_string(generation))\n",
    "    \n",
    "    \n",
    "for ep in range(epoch):\n",
    "\n",
    "    perm = np.random.permutation(len(minbatches)).tolist() \n",
    "    stime=time()\n",
    "    \n",
    "    for k in range(len(minbatches)):\n",
    "        \n",
    "        minbatch = minbatches[perm[k]]\n",
    "        x_padded = utils.make_mask(minbatch)\n",
    "        inp.set(x_padded)\n",
    "        loss, score = BuildModel();\n",
    "        #print(inp.shape)\n",
    "        edf.Forward()\n",
    "        edf.Backward(loss)\n",
    "        edf.GradClip(10)\n",
    "        edf.SGD(eta)\n",
    "       \n",
    "    duration = (time() - stime)/60.\n",
    "    \n",
    "    perp, loss = Eval(valid_data, vacnt)\n",
    "    print(\"Epoch %d: Perplexity: %0.5f Avg loss = %0.5f [%.3f mins]\" % (ep, perp, loss, duration))\n",
    "    \n",
    "    # generate some text given the prefix and trained model\n",
    "    prefix = 'the agreements bring'  \n",
    "    generation = Predict(400, utils.to_idxs(prefix))\n",
    "    print(\"Epoch %d: generated sentence \" % ep)\n",
    "    print (utils.to_string(generation)) \n",
    "\n",
    "    if loss < best_loss:\n",
    "        \n",
    "        best_loss = loss\n",
    "        # save the model\n",
    "        f = open(model, 'wb')\n",
    "        p_value = []\n",
    "        for p in parameters:\n",
    "            p_value.append(p.value)\n",
    "        pickle.dump(p_value, f)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # load the last best model and decay the learning rate\n",
    "        eta *= decay\n",
    "        with open(model, 'rb') as f:\n",
    "            p_value = pickle.load(f)\n",
    "            idx = 0\n",
    "            for p in p_value:\n",
    "                parameters[idx].value = p\n",
    "                idx += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
